{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QocAs69YyCXi"
   },
   "source": [
    "## 安裝 gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jhcSeRMix_tD",
    "outputId": "55c0a581-df44-4d87-fbf7-abe6190d4974"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: \"'gym[all]'\"\n"
     ]
    }
   ],
   "source": [
    "!pip install 'gym[all]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRZsxmmYyIB8"
   },
   "source": [
    "## 程式 4.1: 列出OpenAI Gym中所有的環境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vw9b02q3tv5c",
    "outputId": "70f59942-7380-4fb9-a732-2d2de7412587"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-6446eb3432ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgym\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0menvs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0menvs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregistry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "from gym import envs\n",
    "envs.registry.all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPoXGeOmyN0p"
   },
   "source": [
    "## 程式 4.2： 在OpenAI Gym中創建環境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ghh-oK5PukoI"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0') #創建一個新環境，命名為env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0ug1LniySUc"
   },
   "source": [
    "## 程式 4.3： 在Cartpole中執行動作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zWphEIY6umLW"
   },
   "outputs": [],
   "source": [
    "state1 = env.reset() #初始化環境\n",
    "action = env.action_space.sample() #利用sample()隨機從動作空間中取得一個動作\n",
    "state, reward, done, info = env.step(action) #利用step()執行所選擇的動作，並傳回狀態資料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_unaYBByV1l"
   },
   "source": [
    "## 程式 4.4： 建構策略網路模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jyjjfjj0uqzO"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    " \n",
    "L1 = 4 #輸入資料的向量長度\n",
    "L2 = 150 #隱藏層會輸出長度為150的向量\n",
    "L3 = 2 #策略網路所輸出的向量長度\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(L1, L2), #隱藏層的shape為（L1,L2)\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(L2, L3), #輸出層的shape為（L2,L3)\n",
    "    torch.nn.Softmax(dim=0)   #用softmax()將動作價值轉換成機率\n",
    ")\n",
    "learning_rate = 0.009\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7N0kzZfIyZTp"
   },
   "source": [
    "## 程式 4.5： 使用策略網路來進行隨機動作選擇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPam-bonuuA3"
   },
   "outputs": [],
   "source": [
    "pred = model(torch.from_numpy(state1).float()) #呼叫策略網路模型以產生各動作的機率分佈（命名為pred)\n",
    "action = np.random.choice(np.array([0,1]), p=pred.data.numpy()) #根據策略網路輸出的機率分佈pred來隨機選取動作\n",
    "state2, reward, done, info = env.step(action) #執行動作並記錄不同資料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJhPbhosymuP"
   },
   "source": [
    "## 程式 4.6： 計算折扣回報值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vzy10cF2uywY"
   },
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma=0.99):\n",
    "    lenr = len(rewards) #計算遊戲進行了幾步\n",
    "    disc_return = torch.pow(gamma,torch.arange(lenr).float()) * rewards #計算呈指數方式下降的折扣回報陣列\n",
    "    disc_return /= disc_return.max() #正規化以上的陣列，使其元素的值落在 [0,1]\n",
    "    return disc_return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtvisLILyrJK"
   },
   "source": [
    "## 程式 4.7： 定義損失函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N7aQTK6Eu1Yi"
   },
   "outputs": [],
   "source": [
    "def loss_fn(preds, r): #損失函數的輸入參數為：折扣回報陣列以及機率陣列\n",
    "    return -1 * torch.sum(r * torch.log(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24xz-YFVyuqw"
   },
   "source": [
    "## 程式 4.8： REINFORCE演算法的訓練迴圈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TiwqNkFau3tQ"
   },
   "outputs": [],
   "source": [
    "MAX_DUR = 200 #每場遊戲的最大步數\n",
    "MAX_EPISODES = 500 #訓練的遊戲次數\n",
    "gamma = 0.99\n",
    "score = [] #創立一個串列來記錄每場遊戲的長度\n",
    "\n",
    "for episode in range(MAX_EPISODES):\n",
    "    curr_state = env.reset()\n",
    "    done = False\n",
    "    transitions = [] #使用串列記錄狀態、動作、回饋值（即經驗）    \n",
    "    for t in range(MAX_DUR):\n",
    "        act_prob = model(torch.from_numpy(curr_state).float()) #用模型預測各動作的機率分佈\n",
    "        action = np.random.choice(np.array([0,1]), p=act_prob.data.numpy()) #參照機率分佈隨機選擇一個動作\n",
    "        prev_state = curr_state\n",
    "        curr_state, _, done, info = env.step(action) #在環境中執行動作，並取得新狀態以及是否結束的訊息\n",
    "        transitions.append((prev_state, action, t+1)) #將當前狀態的資料記錄下來（編註：每一輪遊戲後，會把回饋值遞增並存進串列，接下來會將這個串列反轉 (由遞增改為遞減排列) 為回報張量）\n",
    "        if done: #如果輸掉了遊戲，就跳出迴圈 \n",
    "            break\n",
    "    ep_len = len(transitions) #取得整場遊戲的長度\n",
    "    score.append(ep_len)\n",
    "    reward_batch = torch.Tensor([r for (s,a,r) in transitions]).flip(dims=(0,)) #將整場遊戲中的所有回饋值記錄到一個張量中，flip()可將指定階 (第0階) 中的元素進行反轉 (前後順序對調)\n",
    "    disc_returns = discount_rewards(reward_batch) #計算折扣回報陣列\n",
    "    state_batch = torch.Tensor([s for (s,a,r) in transitions]) #將整場遊戲中的所有狀態記錄到一個張量中\n",
    "    action_batch = torch.Tensor([a for (s,a,r) in transitions]) #將整場遊戲中執行過的所有動作記錄到一個張量中\n",
    "    pred_batch = model(state_batch) #重新計算所有狀態下各動作的機率分佈\n",
    "    prob_batch = pred_batch.gather(dim=1,index=action_batch.long().unsqueeze(dim=1)).squeeze() #找出所執行動作對應到的機率值,gather()及unsqueeze()已在第3.3.2小節的小編補充中介紹過了\n",
    "    loss = loss_fn(prob_batch, disc_returns)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2oiK6tDy0Dg"
   },
   "source": [
    "## 程式 4.9： 計算移動平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cgFCXTqCu34q"
   },
   "outputs": [],
   "source": [
    "def running_mean(x, N=50):\n",
    "    kernel = np.ones(N)\n",
    "    conv_len = x.shape[0]-N\n",
    "    y = np.zeros(conv_len)\n",
    "    for i in range(conv_len):\n",
    "        y[i] = kernel @ x[i:i+N]\n",
    "        y[i] /= N\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5WsRhZmy4QR"
   },
   "source": [
    "## 程式 4.10： 將移動平均值畫成圖形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w3UqCl9Tu-rQ",
    "outputId": "aa6e7de6-5917-450e-c700-2f2e00b28187"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "score = np.array(score)\n",
    "avg_score = running_mean(score, 20)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.ylabel(\"Episode Duration\",fontsize=22)\n",
    "plt.xlabel(\"Training Epochs\",fontsize=22)\n",
    "plt.plot(avg_score, color='green')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "F1384_第4章.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
